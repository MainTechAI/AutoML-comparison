{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dask.distributed import Client,LocalCluster\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# *Airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '//home//maraks//Desktop//datasets//binary//airlines.csv'\n",
    "data = pd.read_csv(path) \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if(column in ['Flight','DayOfWeek','Time','Length'] ):\n",
    "        data[column] = pd.to_numeric(data[column])\n",
    "    elif(column in ['Airline','AirportFrom','AirportTo','Delay'] ):\n",
    "        data[column] = data[column].astype('category')\n",
    "        \n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(data[column])\n",
    "        data[column] = le.transform(data[column]) \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client,LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=1,\n",
    "                       threads_per_worker=4,\n",
    "                       memory_limit='6GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X,y = data.iloc[:,:-1] , data.iloc[:,-1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                train_size=0.75, test_size=0.25, random_state=42)\n",
    "y_train=y_train.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "tpot = TPOTClassifier(                  \n",
    "    generations=7, population_size=15, offspring_size=None, \n",
    "    mutation_rate=0.9, crossover_rate=0.1, scoring='neg_log_loss',                                   \n",
    "    config_dict=None, cv=5, use_dask=True, n_jobs=1,\n",
    "    max_eval_time_mins=60, verbosity=2, random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -0.611405 logloss\n",
    "#Best pipeline: RandomForestClassifier(MinMaxScaler(input_matrix), bootstrap=True, criterion=gini, max_features=0.2, min_samples_leaf=8, min_samples_split=4, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_).to_csv('1best.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# *Amazon employee access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '//home//maraks//Desktop//datasets//binary//Amazon_employee_access.csv'\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "\n",
    "    if(column in ['target'] ):\n",
    "        data[column] = data[column].astype('category')\n",
    "    elif(column in ['RESOURCE','MGR_ID','ROLE_ROLLUP_1','ROLE_ROLLUP_2',\n",
    "                'ROLE_DEPTNAME','ROLE_TITLE','ROLE_FAMILY_DESC', \n",
    "                'ROLE_FAMILY', 'ROLE_CODE'] ):\n",
    "        data[column] = pd.to_numeric(data[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client,LocalCluster\n",
    "\n",
    "\n",
    "cluster = LocalCluster(n_workers=1, \n",
    "                       threads_per_worker=4,\n",
    "                       memory_limit='6GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "#client = Client(n_workers=4, threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X,y = data.iloc[:,:-1] , data.iloc[:,-1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                train_size=0.75, test_size=0.25, random_state=42)\n",
    "y_train=y_train.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "tpot = TPOTClassifier(                  \n",
    "                      generations=7, \n",
    "                      population_size=15,\n",
    "                      offspring_size=None, \n",
    "                      mutation_rate=0.9,\n",
    "                      crossover_rate=0.1,\n",
    "                      scoring='neg_log_loss',\n",
    "                      config_dict=None,\n",
    "                      cv=5,\n",
    "                      use_dask=True,\n",
    "                      n_jobs=1,\n",
    "                      max_eval_time_mins=2,\n",
    "                      verbosity=2, \n",
    "                      random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -0.1703658\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# *blood-transfusion-service-center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '//home//maraks//Desktop//datasets//binary//blood-transfusion-service-center.csv'\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "\n",
    "    if(column in ['V1','V2','V3','V4'] ):\n",
    "        data[column] = pd.to_numeric(data[column])\n",
    "    elif(column in ['Class'] ):\n",
    "        data[column] = data[column].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#data.rename(columns={'Class': 'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#data['class'] = data['class'].map({1:-1,2:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client,LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=1, \n",
    "                       threads_per_worker=4,\n",
    "                       memory_limit='6GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "#client = Client(n_workers=4, threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X,y = data.iloc[:,:-1] , data.iloc[:,-1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                train_size=0.75, test_size=0.25, random_state=42)\n",
    "y_train=y_train.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "tpot = TPOTClassifier(                  \n",
    "                      generations=7, \n",
    "                      population_size=15,\n",
    "                      offspring_size=None, \n",
    "                      mutation_rate=0.9,\n",
    "                      crossover_rate=0.1,\n",
    "                      scoring='neg_log_loss',\n",
    "                      config_dict=None,\n",
    "                      cv=5,\n",
    "                      use_dask=True,\n",
    "                      n_jobs=1,\n",
    "                      max_eval_time_mins=2,\n",
    "                      verbosity=2, \n",
    "                      random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_3.py')\n",
    "\n",
    "#from shutil import rmtree\n",
    "#rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -0.469163\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_) # 6 x 114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#tpot.pareto_front_fitted_pipelines_ # verb = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tpot.op_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# *Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '//home//maraks//Desktop//datasets//binary//cifar-10-binary.csv'\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if(column in ['class'] ):\n",
    "        data[column] = data[column].astype('category')\n",
    "    else:\n",
    "        data[column] = pd.to_numeric(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=1,\n",
    "                       threads_per_worker=4,\n",
    "                       memory_limit='6GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X,y = data.iloc[:,:-1] , data.iloc[:,-1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                train_size=0.75, test_size=0.25, random_state=42)\n",
    "y_train=y_train.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "tpot = TPOTClassifier(                  \n",
    "                      generations=7, \n",
    "                      population_size=15,\n",
    "                      offspring_size=None, \n",
    "                      mutation_rate=0.9,\n",
    "                      crossover_rate=0.1,\n",
    "                      scoring='neg_log_loss',\n",
    "                      config_dict=None,\n",
    "                      cv=5,\n",
    "                      use_dask=True,\n",
    "                      n_jobs=1,\n",
    "                      max_eval_time_mins=2,\n",
    "                      verbosity=2, \n",
    "                      random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_3.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "-0.450685\n",
    "-0.450777\n",
    "-0.457732 \n",
    "-0.5139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# *Connect-4-balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '//home//maraks//Desktop//datasets//binary//connect-4-balanced-binary.csv'\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    data[column] = pd.to_numeric(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# stable\n",
    "cluster = LocalCluster(n_workers=1,\n",
    "                       threads_per_worker=4,\n",
    "                       memory_limit='6GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# stable\n",
    "X,y = data.iloc[:,:-1] , data.iloc[:,-1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                train_size=0.75, test_size=0.25, random_state=42)\n",
    "y_train=y_train.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "tpot = TPOTClassifier(                  \n",
    "                      generations=3, \n",
    "                      population_size=5,\n",
    "                      offspring_size=None, \n",
    "                      mutation_rate=0.9,\n",
    "                      crossover_rate=0.1,\n",
    "                      scoring='neg_log_loss',\n",
    "                      config_dict=None,\n",
    "                      cv=5,\n",
    "                      use_dask=True,\n",
    "                      n_jobs=1,\n",
    "                      max_eval_time_mins=2,\n",
    "                      verbosity=2, \n",
    "                      random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_3.py')\n",
    "\n",
    "# 10/20 [06:04<08:49, 52.93s/pipeline] no Dask, 1 job\n",
    "# 11/20 [02:54<05:07, 34.21s/pipeline]  no Dask, 4 job ( crash 1 - Current best internal CV score: -inf)\n",
    "\n",
    "# 10/20 [04:58<03:48, 22.84s/pipeline] Dask, 1 worker, 1 job\n",
    "# 10/20 [01:47<01:19, 7.90s/pipeline] - Dask, 1 worker, 4 jobs (best result)\n",
    "\n",
    "# CV internal score - 0.8642350333749977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -0.361084 logloss\n",
    "\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# *Connect-4-imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '//home//maraks//Desktop//datasets//binary//connect-4-imbalanced-binary.csv'\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    data[column] = pd.to_numeric(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=1,\n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='6GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X,y = data.iloc[:,:-1] , data.iloc[:,-1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                train_size=0.75, test_size=0.25, random_state=42)\n",
    "y_train=y_train.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "tpot = TPOTClassifier(                  \n",
    "                      generations=3, \n",
    "                      population_size=5,\n",
    "                      offspring_size=None, \n",
    "                      mutation_rate=0.9,\n",
    "                      crossover_rate=0.1,\n",
    "                      scoring='neg_log_loss',\n",
    "                      config_dict=None,\n",
    "                      cv=5,\n",
    "                      use_dask=True,\n",
    "                      n_jobs=1,\n",
    "                      max_eval_time_mins=2,\n",
    "                      verbosity=2, \n",
    "                      random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_3.py')\n",
    "\n",
    "# 0.8596326115476478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#-0.403765\n",
    "#-0.419837 \n",
    "#-0.524874\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# *Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '//home//maraks//Desktop//datasets//binary//Fashion-MNIST-binary.csv'\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "        data[column] = pd.to_numeric(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=1,\n",
    "                       threads_per_worker=4,\n",
    "                       memory_limit='6GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X,y = data.iloc[:,:-1] , data.iloc[:,-1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                train_size=0.75, test_size=0.25, random_state=42)\n",
    "y_train=y_train.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "tpot = TPOTClassifier(                  \n",
    "                      generations=3, \n",
    "                      population_size=5,\n",
    "                      offspring_size=None, \n",
    "                      mutation_rate=0.9,\n",
    "                      crossover_rate=0.1,\n",
    "                      scoring='neg_log_loss',\n",
    "                      config_dict=None,\n",
    "                      cv=5,\n",
    "                      use_dask=True,\n",
    "                      n_jobs=1,\n",
    "                      max_eval_time_mins=2,\n",
    "                      verbosity=2, \n",
    "                      random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_3.py')\n",
    "#  5/20 [04:01<09:00, 36.01s/pipeline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -0.29863017411905346\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# *Jungle chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '//home//maraks//Desktop//datasets//binary//jungle_chess_2pcs_raw_endgame_complete-binary.csv'\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    data[column] = pd.to_numeric(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=1,\n",
    "                       threads_per_worker=4,\n",
    "                       memory_limit='6GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X,y = data.iloc[:,:-1] , data.iloc[:,-1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                train_size=0.75, test_size=0.25, random_state=42)\n",
    "y_train=y_train.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "tpot = TPOTClassifier(                  \n",
    "                      generations=3, \n",
    "                      population_size=5,\n",
    "                      offspring_size=None, \n",
    "                      mutation_rate=0.9,\n",
    "                      crossover_rate=0.1,\n",
    "                      scoring='neg_log_loss',\n",
    "                      config_dict=None,\n",
    "                      cv=5,\n",
    "                      use_dask=True,\n",
    "                      n_jobs=1,\n",
    "                      max_eval_time_mins=2,\n",
    "                      verbosity=2, \n",
    "                      random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_3.py')\n",
    "\n",
    "# 0.9058985751493795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#-0.31102109254766236\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# *kc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '//home//maraks//Desktop//datasets//binary//kc1.csv'\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    data[column] = pd.to_numeric(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=1,\n",
    "                       threads_per_worker=4,\n",
    "                       memory_limit='6GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X,y = data.iloc[:,:-1] , data.iloc[:,-1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                train_size=0.75, test_size=0.25, random_state=42)\n",
    "y_train=y_train.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "tpot = TPOTClassifier(                  \n",
    "                      generations=3, \n",
    "                      population_size=5,\n",
    "                      offspring_size=None, \n",
    "                      mutation_rate=0.9,\n",
    "                      crossover_rate=0.1,\n",
    "                      scoring='neg_log_loss',\n",
    "                      config_dict=None,\n",
    "                      cv=5,\n",
    "                      use_dask=True,\n",
    "                      n_jobs=1,\n",
    "                      max_eval_time_mins=2,\n",
    "                      verbosity=2, \n",
    "                      random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_3.py')\n",
    "\n",
    "# 0.8608533322684983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#-0.3435855\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# *KDDCup09_appetency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '//home//maraks//Desktop//datasets//binary//KDDCup09_appetency.csv'\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if(column in ['Var191','Var192','Var193','Var194','Var195',\n",
    "    'Var196','Var197','Var198','Var199','Var200','Var201','Var202',\n",
    "    'Var203','Var204','Var205','Var206','Var207','Var208',\n",
    "    'Var210','Var211','Var212','Var213','Var214','Var215','Var216',\n",
    "    'Var217','Var218','Var219','Var220','Var221','Var222','Var223',\n",
    "    'Var224','Var225','Var226','Var227','Var228','Var229','APPETENCY'] ):\n",
    "        data[column] = data[column].replace('?','NaN')\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(data[column])\n",
    "        data[column] = le.transform(data[column])\n",
    "        #le.inverse_transform([0, 0, 1, 2])\n",
    "        #data[column] = data[column].astype('category')      \n",
    "    else:\n",
    "        data[column] = data[column].replace('?',-1)\n",
    "        data[column] = pd.to_numeric(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=1,\n",
    "                       threads_per_worker=2,\n",
    "                       memory_limit='6GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X,y = data.iloc[:,:-1] , data.iloc[:,-1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                train_size=0.75, test_size=0.25, random_state=42)\n",
    "y_train=y_train.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "tpot = TPOTClassifier(                  \n",
    "                      generations=3, \n",
    "                      population_size=5,\n",
    "                      offspring_size=None, \n",
    "                      mutation_rate=0.9,\n",
    "                      crossover_rate=0.1,\n",
    "                      scoring='neg_log_loss',\n",
    "                      config_dict=None,\n",
    "                      cv=5,\n",
    "                      use_dask=True,\n",
    "                      n_jobs=1,\n",
    "                      max_eval_time_mins=2,\n",
    "                      verbosity=2, \n",
    "                      random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_3.py')\n",
    "\n",
    "# Worker exceeded 95% memory budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# *Vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '//home//maraks//Desktop//datasets//binary//vehicle-binary.csv'\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['Class'] = data['Class'].map({'other':0,'saab':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    data[column] = pd.to_numeric(data[column])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=1,\n",
    "                       threads_per_worker=4,\n",
    "                       memory_limit='6GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X,y = data.iloc[:,:-1] , data.iloc[:,-1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                train_size=0.75, test_size=0.25, random_state=42)\n",
    "y_train=y_train.values.ravel()\n",
    "y_test=y_test.values.ravel()\n",
    "\n",
    "tpot = TPOTClassifier(                  \n",
    "                      generations=3, \n",
    "                      population_size=5,\n",
    "                      offspring_size=None, \n",
    "                      mutation_rate=0.9,\n",
    "                      crossover_rate=0.1,\n",
    "                      scoring='neg_log_loss',\n",
    "                      config_dict=None,\n",
    "                      cv=5,\n",
    "                      use_dask=True,\n",
    "                      n_jobs=1,\n",
    "                      max_eval_time_mins=2,\n",
    "                      verbosity=2, \n",
    "                      random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_3.py')\n",
    "\n",
    "# 0.7507692307692307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -0.515403\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tpot.evaluated_individuals_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from smac.tae import StatusType\n",
    "import autosklearn.classification\n",
    "\n",
    "\n",
    "def show_results(automl):\n",
    "    def get_runhistory_models_performance(automl):\n",
    "        metric = cls.automl_._metric\n",
    "        data = automl.automl_.runhistory_.data\n",
    "        performance_list = []\n",
    "        for run_key, run_value in data.items():\n",
    "            if run_value.status != StatusType.SUCCESS:\n",
    "                # Ignore crashed runs\n",
    "                continue\n",
    "            # Alternatively, it is possible to also obtain the start time with ``run_value.starttime``\n",
    "            endtime = pd.Timestamp(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(run_value.endtime)))\n",
    "            val_score = metric._optimum - (metric._sign * run_value.cost)\n",
    "            test_score = metric._optimum - (metric._sign * run_value.additional_info['test_loss'])\n",
    "            train_score = metric._optimum - (metric._sign * run_value.additional_info['train_loss'])\n",
    "            performance_list.append({\n",
    "                'Timestamp': endtime,\n",
    "                'single_best_optimization_score': val_score,\n",
    "                'single_best_test_score': test_score,\n",
    "                'single_best_train_score': train_score,\n",
    "            })\n",
    "        return pd.DataFrame(performance_list)\n",
    "\n",
    "    ensemble_performance_frame = pd.DataFrame(automl.automl_.ensemble_performance_history)\n",
    "    best_values = pd.Series({'ensemble_optimization_score': -np.inf,\n",
    "                             'ensemble_test_score': -np.inf})\n",
    "    for idx in ensemble_performance_frame.index:\n",
    "        if (\n",
    "            ensemble_performance_frame.loc[idx, 'ensemble_optimization_score']\n",
    "            > best_values['ensemble_optimization_score']\n",
    "        ):\n",
    "            best_values = ensemble_performance_frame.loc[idx]\n",
    "        ensemble_performance_frame.loc[idx] = best_values\n",
    "\n",
    "    individual_performance_frame = get_runhistory_models_performance(automl)\n",
    "    best_values = pd.Series({'single_best_optimization_score': -np.inf,\n",
    "                             'single_best_test_score': -np.inf,\n",
    "                             'single_best_train_score': -np.inf})\n",
    "    for idx in individual_performance_frame.index:\n",
    "        if (\n",
    "            individual_performance_frame.loc[idx, 'single_best_optimization_score']\n",
    "            > best_values['single_best_optimization_score']\n",
    "        ):\n",
    "            best_values = individual_performance_frame.loc[idx]\n",
    "        individual_performance_frame.loc[idx] = best_values\n",
    "\n",
    "\n",
    "    pd.merge(\n",
    "        ensemble_performance_frame,\n",
    "        individual_performance_frame,\n",
    "        on=\"Timestamp\", how='outer'\n",
    "    ).sort_values('Timestamp').fillna(method='ffill').plot(\n",
    "        x='Timestamp',\n",
    "        kind='line',\n",
    "        legend=True,\n",
    "        title='Auto-sklearn accuracy over time',\n",
    "        grid=True,\n",
    "    )\n",
    "    fig = plt.gcf() \n",
    "    fig.set_size_inches(15,8)\n",
    "    plt.show()\n",
    "    return individual_performance_frame, ensemble_performance_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# feature encoding?\n",
    "#titanic['Sex'] = titanic['Sex'].map({'male':0,'female':1})\n",
    "#titanic['Embarked'] = titanic['Embarked'].map({'S':0,'C':1,'Q':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NaNs\n",
    "#titanic = titanic.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# One Hot\n",
    "#from sklearn.preprocessing import MultiLabelBinarizer\n",
    "#mlb = MultiLabelBinarizer()\n",
    "#CabinTrans = mlb.fit_transform([{str(val)} for val in titanic['Cabin'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#pd.get_dummies(data,dummy_na=False).values.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}